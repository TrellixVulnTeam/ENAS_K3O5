# -*- coding: utf-8 -*-
"""ENAS_pytorch_CIFAR100.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v8iPw-OxXtUP_YYPJ_OeyaYVDzqW1iHY
"""

import os
import sys
import time
import random
import glob
import numpy as np
import torch
import logging
import argparse
import torch.nn as nn
import torch.utils
import torch.utils.data as data
import torch.nn.functional as F
#pip3 install pickle5
#import pickle as pickle
import pickle5 as pickle
#import cPickle
import torchvision.transforms as transforms

import utilss
from data import get_loaders
from torch.autograd import Variable
from micro_child import CNN
from micro_controller import Controller
from rps_net import RPS_net_cifar
from cifar_dataset import CIFAR100
from learner import Learner
from util import *
from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig


'''import argparse
ap = argparse.ArgumentParser() 
ap.add_argument("-v", "--video", type=str, help="path to input video file",) 
ap.add_argument("-t", "--tracker", required=False,type=str, default="kcf", help="OpenCV object tracker type") 
ap.add_argument("-f", "--file", required=False) 
args = ap.parse_args()'''
class args1:
    seeds = [456, 23743, 96253, 36, 2543, 14708, 357, 5316, 83, 267]
    test_case = 0
    #inputfiles =  "/content/enas_pytorch/Inputs/cifar100/RPS_CIFAR_M8_J1"
    inputfiles = "/home/sunjanya/test/enas_pytorch/Inputs/cifar100/RPS_CIFAR_M8_J1"
    checkpoint = "results/cifar100"
    #labels_data = "prepare/cifar100_10.pkl"
    #labels_data = "/home/sunjanya/test/enas_pytorch/Inputs/cifar100/RPS_CIFAR_M8_J1/cifar100_shuffled_10_1.pkl"
    labels_data = "/home/sunjanya/test/enas_pytorch/Inputs/cifar100/RPS_CIFAR_M8_J1/cifar100_10_shuffled_10_4.pkl"
    savepoint = ""
    num_tasks = 10
    num_class = 100
    class_per_task = 10
    M = 12
    rigidness_coff = 3 #2.5
    dataset = "CIFAR"
    epochs = 10 #130
    L = 9
    N = 4
    lr = 0.001 #0.001
    train_batch = 10 #16
    test_batch = 10 #16
    workers = 16
    resume = False
    arch = "res-18"
    start_epoch = 0
    evaluate = False
    schedule = [20, 30, 40]
    gamma = 0.5

state = {key: value for key, value in args1.__dict__.items() if not key.startswith('__') and not callable(key)}
print("state:", state)
print("LR:", state['lr'])
print(args1.M)
print("CP", args1.checkpoint)


parser = argparse.ArgumentParser("cifar")
parser.add_argument('--data', type=str, default='~/test/enas_pytorch/Inputs/cifar100/RPS_CIFAR_M8_J1/cifar100_10_shuffled_10_4.pkl', help='location of the data corpus')
parser.add_argument('--batch_size', type=int, default=10, help='batch size') #160
parser.add_argument('--momentum', type=float, default=0.9, help='momentum')
parser.add_argument('--weight_decay', type=float, default=1e-4, help='weight decay')
parser.add_argument('--report_freq', type=float, default=10, help='report frequency')
parser.add_argument('--gpu', type=int, default=0, help='gpu device id')
parser.add_argument('--epochs', type=int, default=10, help='num of training epochs')
parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')
parser.add_argument('--save', type=str, default='EXP', help='experiment name')
#parser.add_argument('--save', type=str, default='WEIGHTS', help='save the weights')
parser.add_argument('--seed', type=int, default=2, help='random seed')

parser.add_argument('--child_lr_max', type=float, default=0.05)
parser.add_argument('--child_lr_min', type=float, default=0.0005)
parser.add_argument('--child_lr_T_0', type=int, default=10)
parser.add_argument('--child_lr_T_mul', type=int, default=2)
parser.add_argument('--child_num_layers', type=int, default=6)
parser.add_argument('--child_out_filters', type=int, default=20)
parser.add_argument('--child_num_branches', type=int, default=5)
parser.add_argument('--child_num_cells', type=int, default=5)
parser.add_argument('--child_use_aux_heads', type=bool, default=False)

parser.add_argument('--controller_lr', type=float, default=0.0035)
parser.add_argument('--controller_tanh_constant', type=float, default=1.10)
parser.add_argument('--controller_op_tanh_reduce', type=float, default=2.5)

parser.add_argument('--lstm_size', type=int, default=64)
parser.add_argument('--lstm_num_layers', type=int, default=1)
parser.add_argument('--lstm_keep_prob', type=float, default=0)
parser.add_argument('--temperature', type=float, default=5.0)

parser.add_argument('--entropy_weight', type=float, default=0.0001)
parser.add_argument('--bl_dec', type=float, default=0.99)

parser.add_argument('-f')
args = parser.parse_args()
print(args)

#args.save = 'search-{}-{}'.format(args, time.strftime("%Y%m%d-%H%M%S"))
utilss.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))

log_format = '%(asctime)s %(message)s'
logging.basicConfig(stream=sys.stdout, level=logging.INFO,
    format=log_format, datefmt='%m/%d %I:%M:%S %p')
fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))
fh.setFormatter(logging.Formatter(log_format))
logging.getLogger().addHandler(fh)


CIFAR_CLASSES = 5

baseline = None
epoch = 0
def main():
    v = args1()
    print("dshghs", v.M)
    print("argv", str(sys.argv))
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        #logging.info('no gpu device available')
        #sys.exit(1)
    #start_sess = (sys.argv[2])
    #start_sess = str(sys.argv[2])
    #test_case = int(sys.argv[1])
    start_sess = 1 #1
    test_case = 1
    v.test_case = test_case
    #labels_data = sys.argv[2]
    labels_data = sys.argv[0]
    v.labels_data = labels_data
    print("SHUFFLED DATA: ", v.labels_data)


    random.seed(args.seed)
    np.random.seed(args.seed)
    
    #torch.set_device(args.cpu)
    '''torch.cuda.set_device(args.gpu)
    torch.backends.cudnn.benchmark = True
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed(args.seed)
    logging.info('gpu device = %d' % args.gpu)
    logging.info("args = %s", args)'''

    seed = args1.seeds[int(test_case)]
    print('seed', seed)
    # Use CUDA
    use_cuda = torch.cuda.is_available()
    random.seed(seed)
    torch.manual_seed(seed)
    if use_cuda:
        torch.cuda.manual_seed_all(seed)

    #model = CNN(args)
    #model = nn.DataParallel(model)
    model = RPS_net_cifar(v)
    model.load_state_dict(torch.load(os.path.join(v.inputfiles, 'init_model_new_%s.pth.tar'%str(v.test_case))))
    model.train()
    #model = model.cuda()
    model.to(device)
    
    print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))

    if not os.path.isdir(v.checkpoint):
      mkdir_p(v.checkpoint)

    if not os.path.isdir("models/CIFAR100/" + v.checkpoint.split("/")[-1]):
        mkdir_p("models/CIFAR100/" + v.checkpoint.split("/")[-1])
    args.savepoint = "models/CIFAR100/" + v.checkpoint.split("/")[-1]

    transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])


    controller = Controller(args)
    controller.cuda()
    baseline = None

    optimizer = torch.optim.SGD(
        model.parameters(),
        args.child_lr_max,
        momentum=args.momentum,
        weight_decay=args.weight_decay,
    )

    controller_optimizer = torch.optim.Adam(
        controller.parameters(),
        args.controller_lr,
        betas=(0.1,0.999),
        eps=1e-3,
    )
    dataloader = CIFAR100

    inds_all_sessions = pickle.load(open(os.path.join(args1.inputfiles, args1.labels_data), 'rb'))
    #inds_all_sessions = pickle.load(open( '/home/sunjanya/test/enas_pytorch/Inputs/cifar100/RPS_CIFAR_M8_J1/cifar100_10_shuffled_10_0.pkl', 'rb'))
    
    
    #for ses in range(start_sess, start_sess + 1):
    for ses in range(v.epochs):
        print("SESS", str(ses))
        print("test_case", str(test_case))
        if(ses == 10):
            break
        path = np.load(v.inputfiles + "/path_new_" + str(ses) + "_" + str(test_case) + ".npy")
        train_path = path.copy()
        infer_path = path.copy()

        print('Starting with session {:d}'.format(ses))
        print('test case : ' + str(test_case))
        print('#################################################################################')
        print("path\n", path)
        print("train_path\n", train_path)
        print("Infer_path\n ", infer_path)
        print("Plssss:\n", inds_all_sessions)
        ind_this_session = inds_all_sessions[ses]
        
        print("dsffdsfdsfdfdf\n",ind_this_session)
        #exit(0)
        #labels = ind_this_session['labels'][0]
        labels = ind_this_session['labels'][0]
        ind_trn = ind_this_session['curent']

        trainset = dataloader(root='/home/sunjanya/test/enas_pytorch/data', train=True, download=False, transform=transform_train, ind=ind_trn)
        trainloader = data.DataLoader(trainset, batch_size=v.train_batch, shuffle=True, num_workers=v.workers)

        ind_tst = ind_this_session['test']
        testset = dataloader(root='/home/sunjanya/test/enas_pytorch/data', train=False, download=False, transform=transform_test, ind=ind_tst)
        testloader = data.DataLoader(testset, batch_size=v.test_batch, shuffle=False, num_workers=v.workers)

        v.sess = ses

        main_learner = Learner(model=model, args=v, trainloader=trainloader,
                               testloader=testloader, labels=labels,
                               use_cuda=device, path=path,
                               train_path=train_path, infer_path=infer_path)
        main_learner.learn()


        cfmat = main_learner.get_confusion_matrix(infer_path)
        np.save(v.checkpoint + "/confusion_matrix_" + str(ses) + "_" + str(test_case) + ".npy", cfmat)
        start_sess += start_sess
        #test_case += test_case
        #if (test_case >= 4):
         # break
    print('done with session {:d}'.format(ses))
    print('#################################################################################')
    while (1):
        if (is_all_done(ses, v.epochs, v.checkpoint)):
            break
        else:
            time.sleep(10)


    train_loader, reward_loader, valid_loader = get_loaders(args)

    scheduler = utilss.LRScheduler(optimizer, args)

    for epoch in range(args.epochs):
        lr = scheduler.update(epoch)
        logging.info('epoch %d lr %e', epoch, lr)

        # training
        print("train accuracy calccccc:")
        train_acc = train(train_loader, model, controller, optimizer,path)
        logging.info('train_acc %f', train_acc)

        train_controller(reward_loader, model, controller, controller_optimizer,path)

        # validation
        print("val accuracy calccccc:")
        valid_acc = infer(valid_loader, model, controller,path)
        logging.info('valid_acc %f', valid_acc)

        utilss.save(model, os.path.join(args.save, 'weights.pt'))
        torch.save(model.state_dict(), "weights.pt")
        #output = torch.load("weights.pt")
        #print(output)


def train(train_loader, model, controller, optimizer,path_main):
    total_loss = utilss.AvgrageMeter()
    total_top1 = utilss.AvgrageMeter()

    for step, (data, target) in enumerate(train_loader):
        model.train()
        #print("Model:", model)
        n = data.size(0)

        data = data.cuda()
        #print("Data:",data)
        target = target.cuda()
        #print("Target:",target)

        optimizer.zero_grad()

        controller.eval()
        #dag, _, _ = controller()
        dag, _, _ = controller()
        #print("Dag:",dag)
        #print("path:", path_main)
        last = -1

        #logits, _ = model(data, dag)
        #logits = model(data, dag, last)
        #print("Before model forward!!!!")
        logits = model(data, path_main, last)
        #print("After model forward!!!!")
        #print("Logits:",logits)
        loss = F.cross_entropy(logits, target)

        loss.backward()
        optimizer.step()

        prec1 = utilss.accuracy(logits, target)[0]
        total_loss.update(loss.item(), n)
        total_top1.update(prec1.item(), n)

        if step % args.report_freq == 0:
            logging.info('train %03d %e %f', step, total_loss.avg, total_top1.avg)

    return total_top1.avg

def train_controller(reward_loader, model, controller, controller_optimizer,path_main):
    global baseline
    total_loss = utilss.AvgrageMeter()
    total_reward = utilss.AvgrageMeter()
    total_entropy = utilss.AvgrageMeter()

    #for step, (data, target) in enumerate(reward_loader):
    for step in range(300):
        data, target = reward_loader.next_batch()
        model.eval()
        n = data.size(0)

        data = data.cuda()
        target = target.cuda()

        controller_optimizer.zero_grad()

        controller.train()
        dag, log_prob, entropy = controller()
        last = -1

        with torch.no_grad():
            #logits, _ = model(data, dag)
            logits = model(data, path_main, last)
            reward = utilss.accuracy(logits, target)[0]

        if args.entropy_weight is not None:
            reward += args.entropy_weight*entropy

        log_prob = torch.sum(log_prob)
        if baseline is None:
            baseline = reward
        baseline -= (1 - args.bl_dec) * (baseline - reward)

        loss = log_prob * (reward - baseline)
        loss = loss.sum()

        loss.backward()

        controller_optimizer.step()

        total_loss.update(loss.item(), n)
        total_reward.update(reward.item(), n)
        total_entropy.update(entropy.item(), n)

        if step % args.report_freq == 0:
            #logging.info('controller %03d %e %f %f', step, loss.item(), reward.item(), baseline.item())
            logging.info('controller %03d %e %f %f', step, total_loss.avg, total_reward.avg, baseline.item())
            #tensorboard.add_scalar('controller/loss', loss, epoch)
            #tensorboard.add_scalar('controller/reward', reward, epoch)
            #tensorboard.add_scalar('controller/entropy', entropy, epoch)

def infer(valid_loader, model, controller,path_main):
    total_loss = utilss.AvgrageMeter()
    total_top1 = utilss.AvgrageMeter()
    model.eval()
    controller.eval()

    with torch.no_grad():
        for step in range(10):
            data, target = valid_loader.next_batch()
            data = data.cuda()
            target = target.cuda()

            dag, _, _ = controller()
            
            last = -1
            #logits, _ = model(data, dag)
            logits = model(data, path_main, last = -1)
            loss = F.cross_entropy(logits, target)

            prec1 = utilss.accuracy(logits, target)[0]
            n = data.size(0)
            total_loss.update(loss.item(), n)
            total_top1.update(prec1.item(), n)

            #if step % args.report_freq == 0:
            logging.info('valid %03d %e %f', step, loss.item(), prec1.item())
            logging.info('normal cell %s', str(dag[0]))
            logging.info('reduce cell %s', str(dag[1]))

    return total_top1.avg


if __name__ == '__main__':
    main()


